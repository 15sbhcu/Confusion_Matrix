{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProbabilityToClassLabel(dataset):\n",
    "    '''\n",
    "    This function determines class label from probability score\n",
    "    '''\n",
    "    cleaned_data1 = dataset.copy()\n",
    "    #Here we are considering class label is +ve if probability score is greater than or equal to 0.5; otherwise \n",
    "    #class label is 0\n",
    "    for i in range(0, len(cleaned_data1)):\n",
    "        if cleaned_data1[i][1] >= 0.5:\n",
    "            cleaned_data1[i][1] = 1\n",
    "        else:\n",
    "            cleaned_data1[i][1] = 0\n",
    "    return cleaned_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Confusion_Matrix(data):\n",
    "    '''\n",
    "    This function computes the four elements of the confusion matrix for binary classification\n",
    "    '''\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    TN = 0\n",
    "    for i in range(0, len(data)):\n",
    "        if data[i][0] == 1 and data[i][1] == 1:\n",
    "            TP += 1\n",
    "        elif data[i][0] == 0 and data[i][1] == 1:\n",
    "            FP += 1\n",
    "        elif data[i][0] == 1 and data[i][1] == 0:\n",
    "            FN += 1\n",
    "        else:\n",
    "            TN += 1\n",
    "    return TP, FN, FP, TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recall(tp, fn):\n",
    "    '''\n",
    "    This function computes the recall given the values of True Positive(TP) and False Negative(FN)\n",
    "    '''\n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Precision(tp, fp):\n",
    "    '''\n",
    "    This function computes the precision given the values of True Positive(TP) and Fasle Positive(FP)\n",
    "    '''\n",
    "    return tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1_Score(precision, recall):\n",
    "    '''\n",
    "    This function computes the F1_score given the values of precision and recall\n",
    "    '''\n",
    "    return 2 * precision * recall / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(tp, fn, fp, tn):\n",
    "    '''\n",
    "    This function computes the accuracy given the values of True Positive(TP), Fasle Negative(FN), False Positive(FP), and True Negative(TN)\n",
    "    '''\n",
    "    return (tp + tn) / (tp + fn + fp + tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClassLabelAccordingToThreshold(probability_score, thresholds):\n",
    "    '''\n",
    "    This function computes the class labels from the threshold values\n",
    "    '''\n",
    "    predicted_class_label = []\n",
    "    for thresh in thresholds: #To iterate through each unique threshold value\n",
    "        lst = []\n",
    "        for val in probability_score:\n",
    "            if val >= thresh:\n",
    "                lst.append(1)\n",
    "            else:\n",
    "                lst.append(0)\n",
    "        predicted_class_label.append(lst) #At the end of the inner loop the list of class labels corressponding to\n",
    "        #a perticular threshold is appended to the predicted_class_label which is a list of lists                \n",
    "    return predicted_class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compute_AUC(dataset, predicted_class_label):\n",
    "    '''\n",
    "    This function computes the AUC value\n",
    "    '''\n",
    "    actual_class_label = dataset[:, 0]  #Getting actual class label of the data points\n",
    "    tpr_lst = [] \n",
    "    fpr_lst = []\n",
    "    for i in range(0, len(predicted_class_label)):\n",
    "        TP, FN, FP, TN = Confusion_Matrix(np.c_[actual_class_label, np.array(predicted_class_label[i])])\n",
    "        TPR = TP / (TP + FN)\n",
    "        FPR = FP / (TN + FP)\n",
    "        tpr_lst.append(TPR)\n",
    "        fpr_lst.append(FPR)\n",
    "    AUC = np.trapz(np.array(tpr_lst), np.array(fpr_lst))\n",
    "    return AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A.Performance metrics for the given data 5_a.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data into a numpy array \n",
    "data_a = np.genfromtxt('5_a.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[       nan        nan]\n",
      " [1.         0.63738662]\n",
      " [1.         0.63516504]\n",
      " ...\n",
      " [1.         0.77772367]\n",
      " [1.         0.84603622]\n",
      " [1.         0.67950667]]\n"
     ]
    }
   ],
   "source": [
    "print(data_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.63738662]\n",
      " [1.         0.63516504]\n",
      " [1.         0.76658559]\n",
      " ...\n",
      " [1.         0.77772367]\n",
      " [1.         0.84603622]\n",
      " [1.         0.67950667]]\n"
     ]
    }
   ],
   "source": [
    "#Deleting the first element which has 'nan' value from the  array  'data_a'\n",
    "cleaned_data_a = np.delete(data_a, 0, 0)\n",
    "print(cleaned_data_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.63738662 0.63516504 0.76658559 ... 0.77772367 0.84603622 0.67950667]\n"
     ]
    }
   ],
   "source": [
    "#Storing probability scores into 'proba_a'\n",
    "proba_a = cleaned_data_a[:, 1]\n",
    "print(proba_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " ...\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "predicted_class_label_from_prob_a = ProbabilityToClassLabel(cleaned_data_a)\n",
    "print(predicted_class_label_from_prob_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP, FN, FP, TN = Confusion_Matrix(predicted_class_label_from_prob_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_a = Recall(TP, FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_a = Precision(TP, FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_score_a = F1_Score(precision_a, recall_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score for the dataset 5_a.csv is  0.9950248756218906\n"
     ]
    }
   ],
   "source": [
    "print('The F1 score for the dataset 5_a.csv is ', F1_score_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the dataset 5_a.csv is  0.9900990099009901\n"
     ]
    }
   ],
   "source": [
    "accuracy_a = Accuracy(TP, FN, FP, TN)\n",
    "print('The accuracy for the dataset 5_a.csv is ', accuracy_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_a = sorted(list(set(proba_a)), reverse = True) #Computing the threshold values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_label_from_threshold_a = ClassLabelAccordingToThreshold(proba_a, thresholds_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC_a = Compute_AUC(cleaned_data_a, predicted_class_label_from_threshold_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC value of the dataset 5_a.csv is  0.48829900000000004\n"
     ]
    }
   ],
   "source": [
    "print('The AUC value of the dataset 5_a.csv is ', AUC_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B.Performance metrics for the given data 5_b.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[       nan        nan]\n",
      " [0.         0.28103453]\n",
      " [0.         0.46515177]\n",
      " ...\n",
      " [0.         0.49933109]\n",
      " [0.         0.15761569]\n",
      " [0.         0.2966183 ]]\n"
     ]
    }
   ],
   "source": [
    "data_b = np.genfromtxt('5_b.csv', delimiter = ',')\n",
    "print(data_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.28103453]\n",
      " [0.         0.46515177]\n",
      " [0.         0.35279298]\n",
      " ...\n",
      " [0.         0.49933109]\n",
      " [0.         0.15761569]\n",
      " [0.         0.2966183 ]]\n"
     ]
    }
   ],
   "source": [
    "#Deleting the first element which has 'nan' value from the  array  'data_a'\n",
    "cleaned_data_b = np.delete(data_b, 0, 0)\n",
    "print(cleaned_data_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.28103453 0.46515177 0.35279298 ... 0.49933109 0.15761569 0.2966183 ]\n"
     ]
    }
   ],
   "source": [
    "proba_b = cleaned_data_b[:, 1]  #Storing probability scores into proba_b\n",
    "print(proba_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " ...\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "predicted_class_label_from_prob_b = ProbabilityToClassLabel(cleaned_data_b)\n",
    "print(predicted_class_label_from_prob_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP, FP, FN, TN = Confusion_Matrix(predicted_class_label_from_prob_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "print(TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "print(FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239\n"
     ]
    }
   ],
   "source": [
    "print(FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9761\n"
     ]
    }
   ],
   "source": [
    "print(TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_b = Recall(TP, FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_b = Precision(TP, FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_score_b = F1_Score(precision_b, recall_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1_score for the dataset 5_b.csv is  0.2791878172588833\n"
     ]
    }
   ],
   "source": [
    "print('The F1_score for the dataset 5_b.csv is ', F1_score_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_b = Accuracy(TP, FN, FP, TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the dataset 5_b.csv is  0.9718811881188119\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy for the dataset 5_b.csv is ', accuracy_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_b = sorted(list(set(proba_b)), reverse = True)  #Computing the thresholds values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_label_from_thresholds_b = ClassLabelAccordingToThreshold(proba_b, thresholds_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC_b = Compute_AUC(cleaned_data_b, predicted_class_label_from_thresholds_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC value for the dataset 5_b is  0.9377570000000001\n"
     ]
    }
   ],
   "source": [
    "print('The AUC value for the dataset 5_b is ', AUC_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C.The code for computing the best threshold of probability which gives lowest values of metric A for the given data 5_c.csv\n",
    "\n",
    "𝐴 = 500 × number of false negative + 100 × numebr of false positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[       nan        nan]\n",
      " [0.         0.45852068]\n",
      " [0.         0.50503693]\n",
      " ...\n",
      " [1.         0.65916054]\n",
      " [1.         0.45626546]\n",
      " [1.         0.65916054]]\n"
     ]
    }
   ],
   "source": [
    "data_c = np.genfromtxt('5_c.csv', delimiter = ',')\n",
    "print(data_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.45852068]\n",
      " [0.         0.50503693]\n",
      " [0.         0.41865174]\n",
      " ...\n",
      " [1.         0.65916054]\n",
      " [1.         0.45626546]\n",
      " [1.         0.65916054]]\n"
     ]
    }
   ],
   "source": [
    "#Deleting the first element which has 'nan' value from the  array  'data_c'\n",
    "cleaned_data_c = np.delete(data_c, 0, 0)\n",
    "print(cleaned_data_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_c = cleaned_data_c[:, 1]  #Storing probability scores into proba_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_c = sorted(list(set(proba_c)), reverse = True) #Computing the thresholds values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_label_from_thresholds_c = ClassLabelAccordingToThreshold(proba_c, thresholds_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = []\n",
    "for i in range(0, len(predicted_class_label_from_thresholds_c)):\n",
    "    TP, FN, FP, TN = Confusion_Matrix(np.c_[cleaned_data_c[:, 0], np.array(predicted_class_label_from_thresholds_c[i])])\n",
    "    A.append(500 * FN + 100 * FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_A = sorted(A) #Sorting A into non decreasing order\n",
    "best_threshold = thresholds_c[A.index(sorted_A[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best threshold of the metric A is  0.2300390278970873\n"
     ]
    }
   ],
   "source": [
    "print('The best threshold of the metric A is ', best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D.The code for computing performance metrics(for regression) for the given data 5_d.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MeanSquaredError(data):\n",
    "    '''\n",
    "    This function computes the mean squared error\n",
    "    '''\n",
    "    sum_squared_error = 0\n",
    "    for i in range(0, len(data)):\n",
    "        sum_squared_error += (data[i][0] - data[i][1]) ** 2 #sum_squared_error accumulates the sum of squared errors\n",
    "    return sum_squared_error / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MeanAbsolutePercentageError(data):\n",
    "    '''\n",
    "    This function computes the mean absolute percentage error\n",
    "    '''\n",
    "    sum_absolute_error = 0\n",
    "    sum_actual_value = 0\n",
    "    for i in range(0, len(data)):\n",
    "        sum_absolute_error += abs(data[i][0] - data[i][1]) #sum_absolute_error accumulates the sum of absolute values of the errors \n",
    "        sum_actual_value += data[i][0]    #sum_actual_value accumulates the sum of the actual values\n",
    "    return sum_absolute_error / sum_actual_value * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeRSquared(data):\n",
    "    '''\n",
    "    This function computes the R squared\n",
    "    '''\n",
    "    actual_sum = 0\n",
    "    for i in range(0, len(data)):\n",
    "        actual_sum += data[i][0]\n",
    "    actual_mean = actual_sum / len(data)  #actual_mean contains the mean of the actual values\n",
    "    ss_total = 0\n",
    "    ss_res = 0\n",
    "    for i in range(0, len(data)):\n",
    "        ss_total += (data[i][0] - actual_mean) ** 2 #ss_total accumulates the sum of squared errors using simple mean model\n",
    "        ss_res += (data[i][0] - data[i][1]) ** 2 #ss_res accumulates the sum of squared errors using the model used for regression\n",
    "    return 1 - ss_res / ss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ nan  nan]\n",
      " [101. 100.]\n",
      " [120. 100.]\n",
      " ...\n",
      " [106.  93.]\n",
      " [105. 101.]\n",
      " [ 81. 104.]]\n"
     ]
    }
   ],
   "source": [
    "#Loading the dataset 5_d.csv into the numpy array data_d\n",
    "data_d = np.genfromtxt('5_d.csv', delimiter = ',')\n",
    "print(data_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101. 100.]\n",
      " [120. 100.]\n",
      " [131. 113.]\n",
      " ...\n",
      " [106.  93.]\n",
      " [105. 101.]\n",
      " [ 81. 104.]]\n"
     ]
    }
   ],
   "source": [
    "#Deleting the first element which has 'nan' value from the  array  'data_a'\n",
    "cleaned_data_d = np.delete(data_d, 0, 0)\n",
    "print(cleaned_data_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error is  177.16569974554707\n"
     ]
    }
   ],
   "source": [
    "mse = MeanSquaredError(cleaned_data_d)\n",
    "print('The mean squared error is ',mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean absolute percentage error is  12.91202994009687\n"
     ]
    }
   ],
   "source": [
    "mape = MeanAbsolutePercentageError(cleaned_data_d)\n",
    "print('The mean absolute percentage error is ',mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R squared error is  0.9563582786990964\n"
     ]
    }
   ],
   "source": [
    "rs = ComputeRSquared(cleaned_data_d)\n",
    "print('The R squared error is ',rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
